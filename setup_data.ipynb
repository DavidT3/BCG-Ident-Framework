{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffef524-0c52-4f0e-bbb9-e8fe28efef7a",
   "metadata": {},
   "source": [
    "# Setting up for BCG identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6f7e9-0765-4fb4-bd07-c5a534aec00e",
   "metadata": {},
   "source": [
    "If you're a student working on a BCG identification project, there is no need for you to run this notebook!\n",
    "\n",
    "You're welcome to read through and get a handle on what the setup processes are doing though - the point of this notebook is essentially to make sure that all optical, infrared, and X-ray images are generated/downloaded ready for you to look at. The 'spot the BCG' notebook (the first of the set) will explain everything you need to know about those different wavelengths, and what we use them for.\n",
    "\n",
    "**If you're running this to setup a BCG identification run, make sure to go and configure everything in 'common.py' before running!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d930d1d-c71f-4681-b6c3-10be0b8695de",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bddffd0-5ada-4ad5-854b-068066985ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ident_run_setup import cosmo, rel_miss, rel_downloaders, side_length, init_samp_file, HISTORY_FILE_PATH, \\\n",
    "    HISTORY_ROOT, load_history, proj_name, update_history, OUTPUT_ROOT, OUTPUT_CLUSTER_PATH\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from astropy.units import Quantity\n",
    "import os\n",
    "from warnings import warn\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from shutil import copyfile\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "try:\n",
    "    from xga.sourcetools import ang_to_rad\n",
    "    from xga.imagetools.misc import pix_deg_scale\n",
    "except:\n",
    "    # Using a general 'except' clause is not usually recommended - you ideally want to be catching\n",
    "    #  specific TYPES of error, so would specify them after the 'except' clause\n",
    "    raise ValueError(\"There was a problem importing XGA - likely running with a pre-multi-mission version.\")\n",
    "\n",
    "# Store the current directory\n",
    "TOP_WD = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94001d-0408-4aef-a5c2-58a1928c40c7",
   "metadata": {},
   "source": [
    "## What cosmology is set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2efd8-3a6c-41ab-a962-4ea8b36f0227",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55784a3-ea71-4dbb-ab96-0b81d7565062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LambdaCDM(name=None, H0=<Quantity 70. km / (Mpc s)>, Om0=0.3, Ode0=0.7, Tcmb0=<Quantity 0. K>, Neff=3.04, m_nu=None, Ob0=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736c93c-9bd7-402b-a31f-3e5ac7a5ed61",
   "metadata": {},
   "source": [
    "## Loading and setting up the sample of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c4d5a-bb7d-4c3d-802b-a9b904a03b02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef468c9-a5e5-45a5-a42f-c3b11e3209f3",
   "metadata": {},
   "source": [
    "### Reading in the sample file and checking for required information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21154f3-6915-4576-9bec-cd700cd77300",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf638c56-5dbe-4686-a076-ddda7ad88137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cent_im_ra</th>\n",
       "      <th>cent_im_dec</th>\n",
       "      <th>redshift</th>\n",
       "      <th>r500</th>\n",
       "      <th>r500-</th>\n",
       "      <th>r500+</th>\n",
       "      <th>r2500</th>\n",
       "      <th>r2500-</th>\n",
       "      <th>r2500+</th>\n",
       "      <th>XCS_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SDSSXCS-124</td>\n",
       "      <td>0.800578</td>\n",
       "      <td>-6.091818</td>\n",
       "      <td>0.247483</td>\n",
       "      <td>1181.028159</td>\n",
       "      <td>21.202221</td>\n",
       "      <td>23.202641</td>\n",
       "      <td>534.834740</td>\n",
       "      <td>7.579124</td>\n",
       "      <td>7.591855</td>\n",
       "      <td>XMMXCS J000312.1-060530.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDSSXCS-2789</td>\n",
       "      <td>0.955540</td>\n",
       "      <td>2.068019</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>1007.860978</td>\n",
       "      <td>17.194150</td>\n",
       "      <td>17.201505</td>\n",
       "      <td>438.706515</td>\n",
       "      <td>5.198301</td>\n",
       "      <td>5.213676</td>\n",
       "      <td>XMMXCS J000349.3+020404.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SDSSXCS-290</td>\n",
       "      <td>2.722639</td>\n",
       "      <td>29.161021</td>\n",
       "      <td>0.348495</td>\n",
       "      <td>913.052256</td>\n",
       "      <td>30.878754</td>\n",
       "      <td>31.209675</td>\n",
       "      <td>412.606577</td>\n",
       "      <td>11.014644</td>\n",
       "      <td>11.199164</td>\n",
       "      <td>XMMXCS J001053.4+290939.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SDSSXCS-1018</td>\n",
       "      <td>4.406325</td>\n",
       "      <td>-0.876192</td>\n",
       "      <td>0.214403</td>\n",
       "      <td>902.259231</td>\n",
       "      <td>22.444665</td>\n",
       "      <td>23.366414</td>\n",
       "      <td>399.213342</td>\n",
       "      <td>6.774208</td>\n",
       "      <td>6.817562</td>\n",
       "      <td>XMMXCS J001737.5-005234.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDSSXCS-134</td>\n",
       "      <td>4.908390</td>\n",
       "      <td>3.609818</td>\n",
       "      <td>0.277304</td>\n",
       "      <td>1123.320736</td>\n",
       "      <td>19.219312</td>\n",
       "      <td>19.225964</td>\n",
       "      <td>510.738163</td>\n",
       "      <td>8.475349</td>\n",
       "      <td>7.419581</td>\n",
       "      <td>XMMXCS J001938.0+033635.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  cent_im_ra  cent_im_dec  redshift         r500      r500-  \\\n",
       "0   SDSSXCS-124    0.800578    -6.091818  0.247483  1181.028159  21.202221   \n",
       "1  SDSSXCS-2789    0.955540     2.068019  0.105285  1007.860978  17.194150   \n",
       "2   SDSSXCS-290    2.722639    29.161021  0.348495   913.052256  30.878754   \n",
       "3  SDSSXCS-1018    4.406325    -0.876192  0.214403   902.259231  22.444665   \n",
       "4   SDSSXCS-134    4.908390     3.609818  0.277304  1123.320736  19.219312   \n",
       "\n",
       "       r500+       r2500     r2500-     r2500+                   XCS_NAME  \n",
       "0  23.202641  534.834740   7.579124   7.591855  XMMXCS J000312.1-060530.5  \n",
       "1  17.201505  438.706515   5.198301   5.213676  XMMXCS J000349.3+020404.8  \n",
       "2  31.209675  412.606577  11.014644  11.199164  XMMXCS J001053.4+290939.6  \n",
       "3  23.366414  399.213342   6.774208   6.817562  XMMXCS J001737.5-005234.2  \n",
       "4  19.225964  510.738163   8.475349   7.419581  XMMXCS J001938.0+033635.3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Point the notebook to the CSV containing basic information about the sample of clusters\n",
    "samp = pd.read_csv(init_samp_file)\n",
    "\n",
    "# A notebook isn't really the ideal environment for this, as each individual cell can be re-run, changing what is stored in \n",
    "#  memory (RAM). As such, we'll run the checks for specific column names in the same cell in which we read in the sample file, so \n",
    "#  no other cell has a chance to modify the dataframe\n",
    "if (~np.isin(np.array(['name', 'cent_im_ra', 'cent_im_dec', 'redshift']), samp.columns)).any():\n",
    "    col_names = \", \".join(samp.columns)\n",
    "    raise KeyError(\"Some required sample columns are not present; input file columns are - {}.\".format(col_names))\n",
    "\n",
    "# Show a snippet of the sample as a quick check, though it must have the required columns to have reached this point\n",
    "samp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d884781-b361-413a-a1ea-554a8634683e",
   "metadata": {},
   "source": [
    "### Adding angular-to-proper distance ratios to sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d8d28-fad0-444a-955a-d2429443557b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af59206-affc-4710-ab6e-382e70dd65c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$[232.85997,~115.803,~295.55704,~208.96598,~252.89891,~269.81895] \\; \\mathrm{\\frac{kpc}{{}^{\\prime}}}$"
      ],
      "text/plain": [
       "<Quantity [232.85996665, 115.80299749, 295.55703817, 208.96598069,\n",
       "           252.89890756, 269.81894828] kpc / arcmin>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ang_step = Quantity(1, 'arcmin')\n",
    "ang_prop_ratios = (ang_to_rad(ang_step, samp['redshift'].values, cosmo=cosmo) / ang_step)\n",
    "# Unfortunately dataframes and quantities do not seem to be getting on at the moment, so we save the\n",
    "#  float version of these values\n",
    "samp['ang_prop_ratio'] = ang_prop_ratios.value\n",
    "ang_prop_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6016340-24b5-4099-bda8-b4dc5319d3a1",
   "metadata": {},
   "source": [
    "## Setting up directory structure and history files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21253f29-cb74-445d-b442-a8015f2c03b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b16d2350-95b3-46d3-9e43-8936b0adb972",
   "metadata": {},
   "source": [
    "### Storage for files that record the history of the identification run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2524f52-caef-44f7-93a0-84b502a7db16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4ed3a3-27ac-4f64-b586-c95815ecf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch the root path to a relative path, otherwise the history is not portable across machines\n",
    "static_samp_file = os.path.relpath(os.path.join(HISTORY_ROOT, 'STATIC_'+proj_name+'.csv'))\n",
    "\n",
    "if os.path.exists(HISTORY_FILE_PATH):\n",
    "    cur_history = load_history()\n",
    "    samp = pd.read_csv(static_samp_file)\n",
    "    ang_prop_ratios = Quantity(samp['ang_prop_ratio'], 'kpc/arcmin')\n",
    "    \n",
    "else:\n",
    "    if not os.path.exists(HISTORY_ROOT):\n",
    "        os.makedirs(HISTORY_ROOT)\n",
    "    \n",
    "    if not os.path.exists(HISTORY_FILE_PATH):\n",
    "        data_ops = {n: {'raw_images': {mn: {'complete': False, 'unavailable': False} for mn in rel_miss}} for n in samp['name'].values}\n",
    "        bcg_ops = {n: {'ident_complete': False} for n in samp['name'].values}\n",
    "        \n",
    "        cur_history = {'project_name': proj_name,\n",
    "                       'num_clusters': len(samp),\n",
    "                       'static_samp_file': static_samp_file,\n",
    "                       'chosen_missions': rel_miss,\n",
    "                       'cosmo_repr': str(cosmo),\n",
    "                       'side_length': side_length.to('kpc').value,\n",
    "                       'data_operations': data_ops,\n",
    "                       'bcg_identification': bcg_ops}\n",
    "        \n",
    "        with open(HISTORY_FILE_PATH, 'w') as write_historo:\n",
    "            json.dump(cur_history, write_historo)\n",
    "        \n",
    "        samp.to_csv(static_samp_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57acd6-eb31-4052-b5b3-ddd2c385508b",
   "metadata": {},
   "source": [
    "### Storage for downloaded/generated raw images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba5624-0d69-48b6-8c00-a149fb4ceaa1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36f94b0-f6d2-4fd4-bd1c-3f077616273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_im_pth_abs = os.path.abspath(\"raw_images/{n}/{m}\") + '/'\n",
    "raw_im_pth = os.path.relpath(raw_im_pth_abs)\n",
    "\n",
    "for src_name in samp['name'].values:\n",
    "    for miss_name in rel_miss:\n",
    "        if not os.path.exists(raw_im_pth.format(m=miss_name, n=src_name)):\n",
    "            os.makedirs(raw_im_pth.format(m=miss_name, n=src_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37bb83-fdd6-43d0-99bc-6d1e2fb71bed",
   "metadata": {},
   "source": [
    "### Storage for outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a75f5-03f7-4e86-831f-46d9fd3fb567",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7431ef-75a3-4812-b558-790439f80a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_name in samp['name'].values:\n",
    "    if not os.path.exists(OUTPUT_CLUSTER_PATH.format(n=src_name)):\n",
    "        os.makedirs(OUTPUT_CLUSTER_PATH.format(n=src_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fa3f0-e291-4198-9fb5-dc02af17b647",
   "metadata": {},
   "source": [
    "## Generating and saving XMM images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef584cef-b004-4a67-a866-9865e101ae3a",
   "metadata": {},
   "source": [
    "We address XMM observations slightly differently, as one of the best ways of getting images is to download the raw data and reprocess it - as this is complex, we also allow for the introduction of already-generated FITS images to the history.\n",
    "\n",
    "Currently, existing file names must be formatted in a very specific way (see the code for further information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70ab1910-ffcd-43d3-a8d4-b9bebe586f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_xmm = None\n",
    "existing_xmm = \"../../useful_data/riley_bcg_ident_xmm_comb_final/\"\n",
    "\n",
    "# The sub-directory path and file name we expect for existing XMM data\n",
    "im_file_existing_xmm = \"{n}/xmm/{n}_0.5-2.0keV_comb.fits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9d4596-b9bd-4f3b-9805-0568f7e8a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving existing XMM images: 100%|██████████████████████████████████████████████| 6/6 [00:00<00:00, 363.13it/s]\n"
     ]
    }
   ],
   "source": [
    "if 'xmm' in rel_miss:\n",
    "    if existing_xmm is not None:\n",
    "\n",
    "        # Check that the existing XMM path is real\n",
    "        if not os.path.exists(existing_xmm):\n",
    "            raise FileNotFoundError(\"The specified 'existing_xmm' path does not exist.\")\n",
    "\n",
    "        # We make a copy of the data operations section of the history, as it may be changed in this next bit\n",
    "        data_op_hist = deepcopy(cur_history['data_operations'])\n",
    "        # And a change flag\n",
    "        hist_change = False\n",
    "        \n",
    "        with tqdm(total=len(samp), desc='Moving existing XMM images') as onwards:\n",
    "            for row_ind, row in samp.iterrows():\n",
    "                # Read out the cluster name - so we can store things in the right place, and keep track of which image\n",
    "                #  belongs to which cluster\n",
    "                cur_name = row['name']\n",
    "                \n",
    "                # First, lets check if this cluster has had XMM data set up already - if yes then we can skip\n",
    "                if cur_history['data_operations'][cur_name]['raw_images']['xmm']['complete']:\n",
    "                    onwards.update(1)\n",
    "                    continue\n",
    "    \n",
    "                # We currently don't build any flexibility into the directory and file name format supported for \n",
    "                #  adding existing images - that will likely change if this gets developed further\n",
    "                cur_exist_path = os.path.join(existing_xmm, im_file_existing_xmm.format(n=cur_name))\n",
    "                if not os.path.exists(cur_exist_path):\n",
    "                    data_op_hist[cur_name]['raw_images']['xmm']['unavailable'] = True\n",
    "                    data_op_hist[cur_name]['raw_images']['xmm']['complete'] = True\n",
    "                    hist_change = True\n",
    "                    \n",
    "                else:\n",
    "                    cur_final_dir = raw_im_pth.format(n=cur_name, m='xmm')\n",
    "                    final_xmm_im_path = os.path.join(cur_final_dir, \"{n}_0.5-2.0keV_comb.fits\".format(n=cur_name))\n",
    "    \n",
    "                    copyfile(cur_exist_path, final_xmm_im_path)\n",
    "                                                     \n",
    "                    # data_op_hist[cur_name]['raw_images']['xmm']['arcsec_per_pix'] = \n",
    "                    # data_op_hist[cur_name]['raw_images']['xmm']['cen_pos'] = \n",
    "                    data_op_hist[cur_name]['raw_images']['xmm']['complete'] = True\n",
    "                    data_op_hist[cur_name]['raw_images']['xmm']['unavailable'] = False\n",
    "                    # Make sure we save the RELATIVE path - otherwise this history file won't work on other systems\n",
    "                    data_op_hist[cur_name]['raw_images']['xmm']['im_path'] = os.path.relpath(final_xmm_im_path)\n",
    "                    hist_change = True\n",
    "                    \n",
    "                onwards.update(1)\n",
    "\n",
    "        # Only run the update history function if there were changes\n",
    "        if hist_change:\n",
    "            cur_history = update_history({'data_operations': data_op_hist})\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(\"The downloading and processing of XMM data for BCG identification \"\\\n",
    "                                  \"projects has not yet been implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae5c89-e2db-49e2-a980-201624dd7aaa",
   "metadata": {},
   "source": [
    "## Downloading DESI Legacy Survey optical/NIR photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08e928-889b-4afd-be9b-19e4b5846a56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503053f-50ba-4e69-abb0-a35435d64086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "ls_pixsize_lim = Quantity(3000, 'pix')\n",
    "\n",
    "#\n",
    "ls_start_pix_scale = Quantity(0.262, 'arcsec/pixel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720e534-9a3d-4a34-be92-6904d8d53574",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c98d3-b102-4ecf-aa89-25872fccccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'desi-ls' in rel_miss:\n",
    "    # Setting up the downloader class instance\n",
    "    desi_down = rel_downloaders['desi-ls']()\n",
    "\n",
    "    ls_pix_side_lengths = ((side_length/ang_prop_ratios)/ls_start_pix_scale).to('pix')\n",
    "\n",
    "    if (ls_pix_side_lengths > ls_pixsize_lim).any():\n",
    "        ls_worst_pix_len = ls_pix_side_lengths.max()\n",
    "        ls_pix_scale_multi = np.ceil(ls_worst_pix_len / ls_pixsize_lim).astype(int)\n",
    "        warn(\"The Legacy Survey pixel side length for one or more clusters is greater than allowed by the cutout \" \\\n",
    "             \"server - changing pixel scale to {}\".format(ls_pix_scale_multi*ls_start_pix_scale), stacklevel=2)\n",
    "    else:\n",
    "        ls_pix_scale_multi = 1\n",
    "\n",
    "    # Set up the final pixel scale\n",
    "    ls_final_pix_scale = ls_start_pix_scale * ls_pix_scale_multi\n",
    "\n",
    "    # Recalculate with the final scale\n",
    "    ls_pix_side_lengths = ((side_length/ang_prop_ratios)/ls_final_pix_scale).to('pix')\n",
    "\n",
    "    # We make a copy of the data operations section of the history, as it may be changed in this next bit\n",
    "    data_op_hist = deepcopy(cur_history['data_operations'])\n",
    "    # And a change flag\n",
    "    hist_change = False\n",
    "    \n",
    "    # Iterating through dataframes like this is a bad idea if you're performing calculations on dataframe information, or if the\n",
    "    #  dataframe is large (more than a few thousand entries), as iterating is very slow. However, we're just iterating to read\n",
    "    #  things out, so it is sort of okay (and quite convenient)\n",
    "    with tqdm(total=len(samp), desc='Downloading DESI Legacy Survey DR10 photometry') as onwards:\n",
    "        for row_ind, row in samp.iterrows():\n",
    "            # Read out the cluster name - so we can store things in the right place, and keep track of which image\n",
    "            #  belongs to which cluster\n",
    "            cur_name = row['name']\n",
    "            \n",
    "            # First, lets check if this cluster has had DESI-LS data downloaded already. We'll use the project \n",
    "            #  history, as it is much computationally cheaper than checking if the expected file names exist\n",
    "            if cur_history['data_operations'][cur_name]['raw_images']['desi-ls']['complete']:\n",
    "                onwards.update(1)\n",
    "                continue\n",
    "\n",
    "            # Reading out relevant information - position is obviously very important, it is where we will center the\n",
    "            #  downloaded images\n",
    "            cur_ra = row['cent_im_ra']\n",
    "            cur_dec = row['cent_im_dec']\n",
    "            # The conversion between arcmin and arcminutes we calculated earlier, note that retrieving from a row\n",
    "            #  of a dataframe like this seems to render it a float again, rather than an Astropy quantity\n",
    "            cur_ang_prop_ratio = Quantity(row['ang_prop_ratio'], 'kpc/arcmin')\n",
    "    \n",
    "            # Though we already calculated the final pixel sizes we need for the LS images, the download method\n",
    "            #  wants the pixel scale and an angular side length specified seperately, so we have to convert the\n",
    "            #  proper side-length to degrees\n",
    "            cur_deg_side_length = (side_length / cur_ang_prop_ratio).to('deg')\n",
    "    \n",
    "            # Populate the raw image storage directory path, with the cluster name and the current mission name\n",
    "            cur_download_dir = raw_im_pth.format(n=cur_name, m='desi-ls')\n",
    "            # Set up the final raw image file name, with some useful info in it\n",
    "            cur_file_name = \"{n}_sidelength{sl}_pixscale{ps}.jpeg\".format(n=cur_name, sl=str(side_length).replace(' ', ''), \n",
    "                                                                          ps=str(ls_final_pix_scale).replace(' ', '').replace('/', 'per'))\n",
    "            # Combine the raw image storage path with the file name\n",
    "            cur_download_pth = os.path.join(cur_download_dir, cur_file_name)\n",
    "    \n",
    "            desi_down.download(ra=cur_ra, dec=cur_dec, bands='griz', layer='ls-dr10', mode='jpeg', autoscale=False, \n",
    "                               ddir=cur_download_dir, size=cur_deg_side_length.value, pixscale=ls_final_pix_scale.value)\n",
    "    \n",
    "            # Change the name of the downloaded file to something more descriptive\n",
    "            down_file_name = 'legacystamps_{r}_{d}_ls-dr10.jpeg'.format(r='{:.6f}'.format(cur_ra), d='{:.6f}'.format(cur_dec))\n",
    "            down_file_path = os.path.join(cur_download_dir, down_file_name)\n",
    "            os.rename(down_file_path, cur_download_pth)\n",
    "\n",
    "            # Now we add to the data operation history - as well as setting the change flag to True so we know to run the \n",
    "            #  history file update at the end of the looping\n",
    "            hist_change = True\n",
    "            data_op_hist[cur_name]['raw_images']['desi-ls']['complete'] = True\n",
    "            data_op_hist[cur_name]['raw_images']['desi-ls']['arcsec_per_pix'] = ls_final_pix_scale.value\n",
    "            # This shouldn't be necessary, as it should be setup so the sample content won't change, but we'll\n",
    "            #  save the central RA and Dec here as well, so all the WCS-related information will be in one place\n",
    "            data_op_hist[cur_name]['raw_images']['desi-ls']['cen_pos'] = [cur_ra, cur_dec]\n",
    "            # Make sure we save the RELATIVE path - otherwise this history file won't work on other systems\n",
    "            data_op_hist[cur_name]['raw_images']['desi-ls']['im_path'] = os.path.relpath(cur_download_pth)\n",
    "            \n",
    "            onwards.update(1)\n",
    "\n",
    "    # Only run the update history function if there were changes\n",
    "    if hist_change:\n",
    "        cur_history = update_history({'data_operations': data_op_hist})\n",
    "\n",
    "# TODO COULD ADD SOME NICE FAIL SAFE SO THAT IT ATTEMPTS TO DOWNLOAD AN SDSS IMAGE IF THERE IS NO LS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9bcc5-28f6-4d4e-b7a5-c4353acb208c",
   "metadata": {},
   "source": [
    "## Downloading VLASS images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72d232-a93f-4968-884e-5a390259e0a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2204e-28de-44a0-97a5-d9c6b2a5806b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'vlass' in rel_miss:\n",
    "    # Setting up the downloader class instance for VLASS - it has fewer download options than the DESI one does\n",
    "    vlass_down = rel_downloaders['vlass'](datatype='se')\n",
    "\n",
    "    # We make a copy of the data operations section of the history, as it may be changed in this next bit\n",
    "    data_op_hist = deepcopy(cur_history['data_operations'])\n",
    "    # And a change flag\n",
    "    hist_change = False\n",
    "\n",
    "    os.chdir('raw_images/')\n",
    "    \n",
    "    # Iterating through dataframes like this is a bad idea if you're performing calculations on dataframe information, or if the\n",
    "    #  dataframe is large (more than a few thousand entries), as iterating is very slow. However, we're just iterating to read\n",
    "    #  things out, so it is sort of okay (and quite convenient)\n",
    "    with tqdm(total=len(samp), desc='Downloading VLASS maps') as onwards:\n",
    "        for row_ind, row in samp.iterrows():\n",
    "            # Read out the cluster name - so we can store things in the right place, and keep track of which image\n",
    "            #  belongs to which cluster\n",
    "            cur_name = row['name']\n",
    "            \n",
    "            # First, lets check if this cluster has had VLASS maps downloaded already. We'll use the project \n",
    "            #  history, as it is much computationally cheaper than checking if the expected file names exist\n",
    "            if cur_history['data_operations'][cur_name]['raw_images']['vlass']['complete']:\n",
    "                onwards.update(1)\n",
    "                continue\n",
    "\n",
    "            # Reading out relevant information - position is obviously very important, it is where we will center the\n",
    "            #  downloaded images\n",
    "            cur_ra = row['cent_im_ra']\n",
    "            cur_dec = row['cent_im_dec']\n",
    "            # The conversion between arcmin and arcminutes we calculated earlier, note that retrieving from a row\n",
    "            #  of a dataframe like this seems to render it a float again, rather than an Astropy quantity\n",
    "            cur_ang_prop_ratio = Quantity(row['ang_prop_ratio'], 'kpc/arcmin')\n",
    "\n",
    "            # Though we already calculated the final pixel sizes we need for the LS images, the download method\n",
    "            #  wants the pixel scale and an angular side length specified seperately, so we have to convert the\n",
    "            #  proper side-length to degrees\n",
    "            cur_deg_side_length = (side_length / cur_ang_prop_ratio).to('deg')\n",
    "    \n",
    "            # Populate the raw image storage directory path, with the cluster name and the current mission name\n",
    "            cur_download_dir = raw_im_pth.format(n=cur_name, m='vlass')\n",
    "            # Set up the final raw image file name, with some useful info in it\n",
    "            cur_file_name = \"{n}_sidelength{sl}_pixscale{ps}.fits\".format(n=cur_name, sl=str(side_length).replace(' ', ''), \n",
    "                                                                          ps='2.5arcsecperpix')\n",
    "            # Combine the raw image storage path with the file name\n",
    "            cur_download_pth = os.path.join(cur_download_dir, cur_file_name)\n",
    "            \n",
    "            # Unfortunately this downloader can raise a general exception rather than a specific one, so I have\n",
    "            #  to account for that\n",
    "            try:\n",
    "                vlass_down.download(ra=cur_ra, dec=cur_dec, ddir=cur_download_dir, size=cur_deg_side_length.value, \n",
    "                                    consider_QA_rejected=True, crop=True)\n",
    "            # I am aware that excepting both is pointless because Exception is the super-class, but it is to remind me\n",
    "            except (HTTPError, Exception) as err:\n",
    "                print(err)\n",
    "                # Record that we tried to get the image, but it wasn't available\n",
    "                hist_change = True\n",
    "                data_op_hist[cur_name]['raw_images']['vlass']['unavailable'] = True\n",
    "                data_op_hist[cur_name]['raw_images']['vlass']['complete'] = True\n",
    "                onwards.update(1)\n",
    "                continue\n",
    "                \n",
    "            # Change the name of the downloaded file to something more descriptive\n",
    "            # down_file_name = 'VLASS_{r}_{d}_'.format(r='{:.6f}'.format(cur_ra), d='{:.6f}'.format(cur_dec))\n",
    "            # poss_vlass_files = [f for f in os.listdir(cur_download_dir) if down_file_name in f]\n",
    "            if len(poss_vlass_files) != 1:\n",
    "                raise ValueError(\"There are multiple possible VLASS images for {n}.\".format(n=cur_name))\n",
    "            else:\n",
    "                down_file_name = poss_vlass_files[0]\n",
    "                \n",
    "            down_file_path = os.path.join(cur_download_dir, down_file_name)\n",
    "            os.rename(down_file_path, cur_download_pth)\n",
    "            \n",
    "            # Now we add to the data operation history - as well as setting the change flag to True so we know to run the \n",
    "            #  history file update at the end of the looping\n",
    "            hist_change = True\n",
    "            data_op_hist[cur_name]['raw_images']['vlass']['complete'] = True\n",
    "            # This shouldn't be necessary, as it should be setup so the sample content won't change, but we'll\n",
    "            #  save the central RA and Dec here as well, so all the WCS-related information will be in one place\n",
    "            data_op_hist[cur_name]['raw_images']['vlass']['cen_pos'] = [cur_ra, cur_dec]\n",
    "            data_op_hist[cur_name]['raw_images']['vlass']['im_path'] = os.path.relpath(cur_download_pth)\n",
    "            \n",
    "            onwards.update(1)\n",
    "\n",
    "    # # Only run the update history function if there were changes\n",
    "    if hist_change:\n",
    "        cur_history = update_history({'data_operations': data_op_hist})\n",
    "\n",
    "os.chdir(TOP_WD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
